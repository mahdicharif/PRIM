{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Graph Networkdisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will provide a short presentation of my work. It led me to create a graph, stored on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import networkdisk as nd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion function (for lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_array(blob):\n",
    "    return json.loads(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(path = \"\"):\n",
    "\n",
    "    start = time.process_time()\n",
    "\n",
    "    sqlite3.register_converter('array', convert_array)\n",
    "    con = sqlite3.connect(path + \"articles_data.db\", detect_types=sqlite3.PARSE_DECLTYPES)\n",
    "\n",
    "    cu = con.cursor()\n",
    "    qu = \"SELECT Name_article, Links FROM Article\"\n",
    "    pe = list(cu.execute(qu))\n",
    "\n",
    "    pe = [(node1, element) for node1, l in pe for element in l]\n",
    "    length = len(pe)\n",
    "\n",
    "    print(\"The data has been imported after {} seconds\".format(time.process_time() - start))\n",
    "    print(\"Its length is {}\".format(length))\n",
    "    \n",
    "    G = nd.sqlite.DiGraph(db=path + \"digraph.db\", nom=\"A\", insert_schema=True, sql_logger=True)\n",
    "\n",
    "    ind = 80000000\n",
    "    next_i = 0\n",
    "    \n",
    "    while next_i < length: # length of the whole db\n",
    "\n",
    "        print(\"beginning to insert the next 80 000 000 edges\")\n",
    "\n",
    "        start = time.process_time()\n",
    "        \n",
    "        dat = pe[next_i:ind]\n",
    "\n",
    "        print(\"data loaded\")\n",
    "\n",
    "        G.add_edges_from(dat)\n",
    "\n",
    "        print(\"{} edges have been added\".format(ind))\n",
    "        print(\"after {} seconds\".format(time.process_time() - start))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        ind += 80000000\n",
    "        next_i += 80000000\n",
    "        dat = None\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    G.helper.db.close()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph(\"/data/name_user/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went over the database of articles by slices of 80.000.000 edges. It enabled me not to provide the whole set of edges (around 300 million) all at once to Networkdisk. \n",
    "\n",
    "The process took about 24h to complete. The result is a database, named *digraph.db*, in which are stored the nodes and edges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
